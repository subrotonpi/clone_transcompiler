{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "dr = '/home/egk204/PycharmProjects/code-trans-aug/'\n",
    "data = pd.read_pickle(dr+'storage/data_original.pkl')\n",
    "\n",
    "#collection of parsable statements from original dataset for random insertion\n",
    "\n",
    "o = data\n",
    "ja = o[o.language_extension=='java']\n",
    "py = o[o.language_extension=='py']\n",
    "\n",
    "px = []\n",
    "for i in py.code:\n",
    "    px.extend(i.split('\\n'))\n",
    "\n",
    "import ast\n",
    "def check_py_ast(c):\n",
    "    try:\n",
    "        ast.parse(c)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "pxp = [s for s in px if check_py_ast(s) ]\n",
    "\n",
    "jx = []\n",
    "for i in ja.code:\n",
    "    jx.extend(i.split(';'))\n",
    "\n",
    "import javalang\n",
    "def check_ja_ast(c):\n",
    "    try:\n",
    "        tokens = javalang.tokenizer.tokenize(c+\";\")\n",
    "        parser = javalang.parser.Parser(tokens)\n",
    "        parser.parse_expression()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "jxp = [s for s in jx if check_ja_ast(s) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class common_transform:\n",
    "    def transform_copy_statement(self):\n",
    "        self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "        self.splitted.insert(self.pos, self.pos-1)\n",
    "        return self.splitted\n",
    "    \n",
    "    def transform_delete_statement(self):\n",
    "        if len(self.splitted)>0:\n",
    "            self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "            self.splitted.pop(self.pos)\n",
    "        return self.splitted\n",
    "    def transform_swap_statement(self):\n",
    "        if len(self.splitted)>=2:\n",
    "            self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "            l = self.splitted\n",
    "            pos1 = self.pos\n",
    "            if self.pos == len(self.splitted)-1:\n",
    "                pos2 = self.pos-1\n",
    "            else:\n",
    "                pos2 = self.pos+1\n",
    "            l[pos1], l[pos2] = l[pos2], l[pos1]\n",
    "            self.splitted = l\n",
    "        return self.splitted\n",
    "    def transform_replace_op(self):\n",
    "        if len(self.splitted)>0:\n",
    "            self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "            s = str(self.splitted[self.pos])\n",
    "            ops = ['+', '-', '*', '/', '>', '<', '<=', '>=','-=','+=']\n",
    "            d = {'+':'-', \n",
    "                '-':'+', \n",
    "                '*':'/', \n",
    "                '/':'*', \n",
    "                '>':'<', \n",
    "                '<':'>', \n",
    "                '<=':'>=', \n",
    "                '>=':'+=',\n",
    "                '-=':'+=',\n",
    "                '+=':'-='         \n",
    "            }\n",
    "            p1 = ops[random.randrange(len(ops))]\n",
    "            p2 = d[p1]\n",
    "            s = s.replace(p1, p2)\n",
    "            self.splitted[self.pos] = s\n",
    "        return self.splitted\n",
    "    \n",
    "    \n",
    "    \n",
    "class python_transform(common_transform):\n",
    "    def __init__(self, code):\n",
    "        self.code = code\n",
    "        self.splitted = code.split('\\n')\n",
    "        self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "    def transform_insert_statement(self):\n",
    "        self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "        #sample statements\n",
    "        statements = [\n",
    "            'a = 10', \n",
    "            'try:\\nprint(x)\\nexcept:\\nprint(\"An exception occurred\")',\n",
    "            'myNumbers = {1, 2, 3};',\n",
    "            'pos1, pos2  = 1, 3'\n",
    "            \n",
    "        ]\n",
    "        #self.splitted.insert(self.pos, statements[random.randrange(len(statements))])\n",
    "        self.splitted.insert(self.pos, random.choice(pxp))\n",
    "        return self.splitted\n",
    "    def transform_insert_comment(self):\n",
    "        self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "        self.splitted.insert(self.pos, '#this is a comment;')\n",
    "        return self.splitted\n",
    "\n",
    "\n",
    "class java_transform(common_transform):\n",
    "    def __init__(self, code):\n",
    "        self.code = code\n",
    "        self.splitted = code.split(';')\n",
    "        self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 ) \n",
    "    #sample statements    \n",
    "    def transform_insert_statement(self):\n",
    "        self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "        #sample\n",
    "        statements =[\n",
    "            'int a = 10;', \n",
    "            'try {//  Block of code to try } catch(Exception e) { //  Block of code to handle errors}',\n",
    "            'int[] myNumbers = {1, 2, 3};',\n",
    "            'System.out.println(myNumbers[10]);',  \n",
    "        ]\n",
    "        \n",
    "        #self.splitted.insert(self.pos, statements[random.randrange(len(statements))])\n",
    "        self.splitted.insert(self.pos, random.choice(jxp))\n",
    "        return self.splitted\n",
    "    \n",
    "    def transform_insert_comment(self):\n",
    "        self.pos = random.randrange(len(self.splitted) if len(self.splitted)>0 else 1 )\n",
    "        self.splitted.insert(self.pos, '//this is a comment;')\n",
    "        return self.splitted\n",
    "    \n",
    "\n",
    "def transform(code, ext):\n",
    "    if ext =='java':\n",
    "        obj = java_transform(code)\n",
    "    if ext =='py':\n",
    "        obj = python_transform(code)\n",
    "    #print(obj.code, obj.splitted, obj.pos)\n",
    "    \n",
    "    #select any transformation\n",
    "    for i in range(0,random.randrange(6)):\n",
    "        options = [0,1,2,3,4,5]\n",
    "        rand_func = random.choice(options)\n",
    "        if rand_func == 0:\n",
    "            c = obj.transform_insert_statement()\n",
    "        elif rand_func == 1:\n",
    "            c = obj.transform_delete_statement()\n",
    "        elif rand_func == 2:\n",
    "            c = obj.transform_insert_comment()\n",
    "        elif rand_func == 3:\n",
    "            c = obj.transform_swap_statement()\n",
    "        elif rand_func == 4:\n",
    "            c = obj.transform_replace_op()\n",
    "        elif rand_func == 5:\n",
    "            c = obj.transform_copy_statement()\n",
    "    c = obj.splitted\n",
    "    #print(type(c))\n",
    "    return '\\n'.join(str(i) for i in c) if ext == 'py' else ';'.join(str(i) for i in c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = []\n",
    "for index, row in data.iterrows():\n",
    "    code_ = row.code\n",
    "    lang_ = row.language_extension\n",
    "    print(row.id)\n",
    "    c_ = transform(code_, lang_)\n",
    "    aug.append(list(row[:2])+[c_]+list(row[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = pd.DataFrame(aug, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug.id = aug.id+38110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aug.to_pickle(dr+'storage/alternative/data_alternative.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dr = '/home/egk204/PycharmProjects/code-trans-aug/'\n",
    "dra = dr+'storage/alternative/'\n",
    "\n",
    "data = pd.read_pickle(dr+'storage/data_original.pkl')\n",
    "aug = pd.read_pickle(dr+'storage/alternative/data_alternative.pkl')\n",
    "\n",
    "org_java = data[data.language_extension=='java']\n",
    "org_py = data[data.language_extension=='py'] #for GMN i have converted in transpiled\n",
    "aug_java = aug[aug.language_extension=='java']\n",
    "aug_py = aug[aug.language_extension=='py'] #for GMN needs to convert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug[['id','language_extension', 'code']].to_csv(dr+'storage/alternative/aug_alt.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([data,aug])\n",
    "mapping = {item:i+1 for i, item in enumerate(merged[\"problem_class\"].unique())}\n",
    "merged[\"task\"] = merged['problem_class'].apply(lambda x: mapping[x])\n",
    "merged.to_pickle(dr+'storage/alternative/codes_alt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clon-non clone pair java - py -large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_aug_java = merged[merged.language_extension=='java']\n",
    "alt_aug_py = merged[merged.language_extension=='py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  filter_for_pair (df):\n",
    "    df = df[['id', 'task']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_for_pair(alt_aug_java).to_csv(dr+'storage/alternative/alt_aug_java_large.csv', header=None, index=False)\n",
    "filter_for_pair(alt_aug_py).to_csv(dr+'storage/alternative/alt_aug_py_large.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_code(df):\n",
    "    cp = 1\n",
    "    cj = 1\n",
    "    refined = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row.language_extension =='py':\n",
    "            if(len(row.code.split('\\n'))>6):\n",
    "                cp+=1\n",
    "                refined.append(list(row))\n",
    "        if row.language_extension =='java':\n",
    "            if(len(row.code.split(';'))>6):\n",
    "                cj+=1\n",
    "                refined.append(list(row))\n",
    "    print(df[df.language_extension =='py'].shape[0]+ df[df.language_extension =='java'].shape[0], pd.DataFrame(refined).shape, (cp+cj)/df.shape[0])\n",
    "    return pd.DataFrame(refined, columns=list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_small = filter_code(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_aug_java_small = merged_small[merged_small.language_extension=='java']\n",
    "alt_aug_py_small = merged_small[merged_small.language_extension=='py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_for_pair(alt_aug_java_small).to_csv(dr+'storage/alternative/alt_aug_java_small.csv', header=None, index=False)\n",
    "filter_for_pair(alt_aug_py_small).to_csv(dr+'storage/alternative/alt_aug_py_small.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "import pandas as pd\n",
    "dr = '~/PycharmProjects/code-trans-aug/'\n",
    "dra = dr+'storage/alternative/'\n",
    "\n",
    "cols = ['id1', 'id2', 'label']\n",
    "clones_sm =pd.read_csv(dr+'storage/alternative/clones_augmented_small_sample.csv', names=cols)\n",
    "print(clones_sm.shape)\n",
    "non_clones_sm =pd.read_csv(dr+'storage/alternative/non_clones_augmented_small_sample.csv', names=cols).sample(clones_sm.shape[0])\n",
    "print(non_clones_sm.shape)\n",
    "da = pd.concat([clones_sm, non_clones_sm]).sample(frac=.6)\n",
    "\n",
    "#small\n",
    "do = pd.read_csv(dr+'/storage/dataset_small/dataset_org.csv', names=cols)#read original small\n",
    "dataset_or_80_20_aug = do.append(da.sample(int(len(do)*0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_or_80_20_aug.to_csv(dr+'storage/alternative/alt_small_dataset_or_80_20_aug.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large #shuf -n\n",
    "import pandas as pd\n",
    "dr = '~/PycharmProjects/code-trans-aug/'\n",
    "\n",
    "cols = ['id1', 'id2', 'label']\n",
    "clones =pd.read_csv(dr+'storage/alternative/clones_augmented_large_sample.csv', names=cols)\n",
    "print(clones.shape)\n",
    "non_clones =pd.read_csv(dr+'storage/alternative/non_clones_augmented_large_sample.csv', names=cols).sample(clones.shape[0])\n",
    "print(non_clones.shape)\n",
    "\n",
    "da = clones.append(non_clones)\n",
    "\n",
    "do = pd.read_csv(dr+'/storage/dataset_large/dataset_org.csv', names=cols)#read original large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_or_80_20_aug = do.append(da.sample(int(len(do)*0.25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_or_80_20_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_or_80_20_aug.to_csv(dr+'storage/alternative/alt_large_dataset_or_80_20_aug.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"~/PycharmProjects/code-trans-aug/\")\n",
    "import pandas as pd\n",
    "storage_dir = '~/PycharmProjects/code-trans-aug/storage/'\n",
    "sv_dir = '~/PycharmProjects/code-trans-aug/storage/alternative/'\n",
    "import pandas as pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure ANTLR features for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "aug = pd.read_csv('aug_alt.csv', names=['id', 'lang', 'code'])\n",
    "for ind, row in aug.iterrows():\n",
    "    fi = 'aug/'+str(row.id) +'.'+ str(row.lang)\n",
    "    with open(fi, 'w') as f:\n",
    "        f.write(str(row.code))\n",
    "        \n",
    "s = '~/PycharmProjects/code-trans-aug/storage/'\n",
    "column_names = ['fpath','variable', 'arguments', 'operators', 'expressions', 'loops','operands', 'exceptions', 'exception_clause', 'mccable_complex']\n",
    "j = pd.read_csv(s+'feat_alt/java_features.csv', names=column_names)\n",
    "p = pd.read_csv(s+'feat_alt/python_features.csv', names=column_names)\n",
    "j['id']=[f.split('/')[-1:][0].split('.')[0]  for f in j.fpath]\n",
    "p['id']=[f.split('/')[-1:][0].split('.')[0]  for f in p.fpath]\n",
    "pd.concat([j.iloc[:,1:], p.iloc[:,1:]]).to_csv(s+'alternative/alt_feat.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats_to_dict(dft, feats_dict):\n",
    "    \n",
    "    col = ['variable', 'arguments', 'operators', 'expressions', 'loops','operands', 'exceptions', 'exception_clause', 'mccable_complex']\n",
    "    for ind, row in dft.iterrows():\n",
    "        key = int(row.id)\n",
    "        value = list(row[col])\n",
    "        feats_dict[key] = value\n",
    "    return feats_dict\n",
    "def get_antlr_feature_dictionary(storage_dir = storage_dir):\n",
    "    # load features \n",
    "    #generate dict   \n",
    "    column_names = ['variable', 'arguments', 'operators', 'expressions', 'loops','operands', 'exceptions', 'exception_clause', 'mccable_complex', 'file_path', 'problem_class','id']\n",
    "    \n",
    "    alt = pd.read_csv(storage_dir+'alternative/alt_feat.csv').sort_values(['id'])\n",
    "    ojf = pd.read_csv(storage_dir+'feat_original/java_features_pc_id.csv', names=column_names)\n",
    "    opf = pd.read_csv(storage_dir+'feat_original/python_features_pc_id.csv', names=column_names)\n",
    "\n",
    "    feats_dict = {}\n",
    "    feats_dfs = [alt, opf, ojf]\n",
    "    for dft in feats_dfs:\n",
    "        feats_dict = feats_to_dict(dft, feats_dict)\n",
    "    print('features dict len:',len(feats_dict))\n",
    "\n",
    "    return feats_dict\n",
    "\n",
    "fd = get_antlr_feature_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df, data_dict):\n",
    "    print('printing data shape, data_dict length',df.shape, len(data_dict))\n",
    "    dataset = []\n",
    "    n = [0]*9\n",
    "    #for df in [clones, non_clones]:\n",
    "    for ind in range (len(df)):\n",
    "        row = list(df.iloc[ind,:])\n",
    "        if data_dict.get(row[0]) is not None:\n",
    "            x1 = data_dict.get(row[0])\n",
    "        else: \n",
    "            x1 = n\n",
    "        if data_dict.get(row[1]) is not None:\n",
    "            x2 =  data_dict.get(row[1])\n",
    "        else:\n",
    "            x2 =n\n",
    "        label = row[2] \n",
    "        dataset.append(x1+x2+[label])\n",
    "    \n",
    "    d = pd.DataFrame(dataset)\n",
    "    d = d.fillna(0)\n",
    "    return d\n",
    "\n",
    "pairs = pd.read_csv(sv_dir+'alt_small_dataset_or_80_20_aug.csv', header=None)\n",
    "dset = get_dataset(pairs, fd)\n",
    "dset = dset[dset.values.sum(axis=1) != 0] \n",
    "dset.to_csv(sv_dir+'alt_ft_lcdsa_small.csv', header=None, index=False)\n",
    "pairs = pd.read_csv(sv_dir+'alt_large_dataset_or_80_20_aug.csv', header=None)\n",
    "dset = get_dataset(pairs, fd)\n",
    "dset = dset[dset.values.sum(axis=1) != 0] \n",
    "dset.to_csv(sv_dir+'alt_ft_clcdsa_large.csv', header=None, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('/home/egk204/PycharmProjects/code-trans-aug/storage/alternative/data_alternative.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    ratios = [8,1,1]\n",
    "\n",
    "    data_num = len(data)\n",
    "    train_split = int(ratios[0] / sum(ratios) * data_num)\n",
    "    val_split = train_split + int(ratios[1] / sum(ratios) * data_num)\n",
    "\n",
    "    data = data.sample(frac=1, random_state=666)\n",
    "    \n",
    "    train_list = data.iloc[:train_split]\n",
    "    valid_list = data.iloc[train_split:val_split]\n",
    "    test_list = data.iloc[val_split:]\n",
    "    \n",
    "    return train_list, valid_list, test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = '~/PycharmProjects/code-trans-aug/storage/alternative/'\n",
    "d_t = 'c4/'\n",
    "cols = ['id1', 'id2', 'label']\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_pairs = pd.read_csv(dr+'alt_large_dataset_or_80_20_aug.csv', names=cols)\n",
    "large = augmented_pairs[augmented_pairs.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = split_dataset(large)\n",
    "train.to_csv(dr+d_t+'train_large.csv',header=None, index=False)\n",
    "\n",
    "test.to_csv(dr+d_t+'test_large.csv',header=None, index=False)\n",
    "\n",
    "dev.to_csv(dr+d_t+'dev_large.csv',header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_pairs = pd.read_csv(dr+'alt_small_dataset_or_80_20_aug.csv', names=cols)\n",
    "small = augmented_pairs[augmented_pairs.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = split_dataset(small)\n",
    "train.to_csv(dr+d_t+'train_small.csv',header=None, index=False)\n",
    "\n",
    "test.to_csv(dr+d_t+'test_small.csv',header=None, index=False)\n",
    "\n",
    "dev.to_csv(dr+d_t+'dev_small.csv',header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to make GMN converted\n",
    "dr = '/home/egk204/PycharmProjects/code-trans-aug/'\n",
    "gmn_original = pd.read_pickle(dr+'storage/dataset_large/gmn_original_final.pkl')\n",
    "gmn_trans_alt = pd.read_pickle(dr+'storage/alternative/alternative_to_java.pkl') #will replace with translated one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {item:i+1 for i, item in enumerate(gmn_trans_alt[\"problem_class\"].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmn = pd.concat([gmn_original, gmn_trans_alt])\n",
    "gmn[\"task\"] = gmn['problem_class'].apply(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmn.to_pickle(dr+'storage/alternative/gmn_codes_alt.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
